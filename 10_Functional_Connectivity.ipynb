{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Connectivity\n",
    "\n",
    "In this tutorial, we'll cover the basics of functional connectivity analyses. In brief, functional connectivity examines the correlations between timeseries of brain regions. Functional connectivity is often analyzed in the context of resting-state fMRI, which often referred to as 'intrinsic' functional connectivity because these correlations arise in the absence of tasks and behaviour.\n",
    "\n",
    "We'll analyze a 20 minute resting-state scan of one subject to explore the methodology and types of analyses we can perform. We'll start by getting an _atlas_ that let's us define non-overlapping cortical regions. Then, we'll extract the _mean timeseries_ of these regions using nilearn's `NiftiLabelsMasker`, which is similar to previous masker objects we've used, except that it directly computes the mean timeseries for our regions -- easy! Next, we'll run a functional connectivity analysis by correlating the timeseries between each possible pairs of regions, examine the resulting connectivity matrix, and discuss how we can begin to analyze this matrix.     \n",
    "\n",
    "**Installing Brain Connectivity Toolbox**\n",
    "\n",
    "But first we need to install an external dependency! [Brain Connectivity Toolbox](https://sites.google.com/site/bctnet/) (BCT) is a powerful MATLAB toolbox for analyzing functional connectivity. There is a Python version of BCT called `bctpy` and the Github can be found here: https://github.com/aestrivex/bctpy (see the Wiki here: https://github.com/aestrivex/bctpy/wiki). We'll be using functions from `bctpy` to analyze connectivity matrices (actual connectivity matrices are computed using nilearn). \n",
    "\n",
    "Unfortunately, this package is fairly niche and is not set up to install using the standard `pip` or `conda` install command. So, you need to download the package from Github directly (`Clone or download` > `Download ZIP`) and **place it in this directory**. Then run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e bctpy-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **restart the notebook kernel** by going to `Kernel` > `Restart Kernel...`. You'll now be able to import `bct`. \n",
    ")\n",
    "In addition to `bct`, we'll also import a few functions/classes from nilearn that will let us first define our regions (`fetch_atlas_schaefer_2018`) and then extract the mean timeseries of these regions (`NiftiLabelsMasker`). We'll also import  `ConnectivityMeasure` from nilearn, which makes it really easy to run a functional connectivity analysis. Finally, we'll import the plotting and image modules entirely rather than their individual functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, special\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import plotting, image\n",
    "import bct\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting an atlas\n",
    "\n",
    "Atlases provide a map of brain regions. Atlases can come in a variety of formats, including a labelled 3D NIfTI image (i.e. each voxel labelled according to its region), a 4D NIfTI image in which each volume is a probabilistic map of a region (each voxel is assigned a probability that it belongs to each region), or a list of coordinates that denote the center of each region. \n",
    "\n",
    "Nilearn has a [number of functions that let us fetch popular atlases](https://nilearn.github.io/modules/reference.html#module-nilearn.datasets). We'll use one of these functions to fetch the Schaefer altas ([link to the paper](https://academic.oup.com/cercor/article/28/9/3095/3978804), which is a recent atlas that determines region boundaries based on sudden changes in voxelwise functional connectivity (called 'gradients') and clustering approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = fetch_atlas_schaefer_2018(n_rois=100, resolution_mm=2)\n",
    "\n",
    "# we can get the image out of the dictionary\n",
    "atlas_img = nib.load(atlas['maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the atlas overlaid on top of a MNI template. The coordinates I've selected highlight some nice features of the atlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(atlas_img, cmap='jet', cut_coords=[6, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can explore the atlas interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(atlas_img, cmap='jet', symmetric_cmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the region labels, in order of numerical label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode just converts it from byte-formate to string\n",
    "labels = [x.decode() for x in atlas['labels']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting regions timeseries from resting state\n",
    "\n",
    "Now that we have an atlas, we want to extract the mean timeseries from each region. We also want to apply some post-processing to these timeseries so that we can remove low-frequency trends in the data (temporal filtering) and regress out sources of noise (i.e. confound regression). \n",
    "\n",
    "Although this may seem daunting (we have 200 regions), it is actually really easy to do this all in one step using nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load in our confounds that we wish to regress out. Remember that confounds are found in `regressor_confounds.tsv` files produced by fmriprep. Instead of loading in the whole file, we'll only load in the confounds that we want. You can see that I have specified more confounds than we are used to seeing (only the 6 motion parameters). We are going to get the 6 motion parameters, their derivatives, their square, and the derivatives of the squares (24 motion parameters total, commonly called Friston24). We'll also include the framewise displacement (a composite measure of motion), and the mean signals of white matter and CSF. The two latter confounds give us signal fluctuations that we presume to be physiological noise that we want to remove from our data.\n",
    "\n",
    "Try also adding 'global_signal' by uncommenting the final line in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_names = ['trans_x', 'trans_y', 'trans_z',\n",
    "                  'trans_x_derivative1', 'trans_y_derivative1', 'trans_z_derivative1',\n",
    "                  'trans_x_power2', 'trans_y_power2', 'trans_z_power2',\n",
    "                  'trans_x_derivative1_power2', 'trans_y_derivative1_power2', 'trans_z_derivative1_power2',\n",
    "                  'rot_x', 'rot_y', 'rot_z', \n",
    "                  'rot_x_derivative1', 'rot_y_derivative1', 'rot_z_derivative1',\n",
    "                  'rot_x_power2', 'rot_y_power2', 'rot_z_power2',\n",
    "                  'rot_x_derivative1_power2', 'rot_y_derivative1_power2', 'rot_z_derivative1_power2', \n",
    "                  'framewise_displacement', 'csf', 'white_matter']\n",
    "\n",
    "# confound_names.append('global_signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = pd.read_table('rs-data/sub-01_task-rest_run-03_desc-confounds_regressors.tsv', usecols=confound_names)\n",
    "confounds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first row has not-a-number (`NaN`) for the derivatives; this is unavoidable because derivatives are the rate of change between data points, so it requires two datapoints by definition. Nilearn unfortunately, but understandably, does not accept `NaN`s so we need to drop that row. This row corresponds to the first functional volume, which now needs to be dropped as well.\n",
    "\n",
    "Now let's load in our functional data, and then drop the first volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_img = nib.load('rs-data/sub-01_task-rest_run-03_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')\n",
    "func_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first volume from image\n",
    "func_img = image.index_img(func_img, slice(1, None))\n",
    "\n",
    "func_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's not forget to drop the first row of our confounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert confounds to numpy array (from DataFrame), and drop first row\n",
    "confounds = confounds.values[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract data. We will use `NiftiLabelsMasker` post-process, apply confound regression, and extract the mean timeseries of each region. This function is very similar to what we have seen before. The result will be a time by region array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our parameters\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_img, high_pass=.01, detrend=True, standardize=True, \n",
    "                           t_r=2)\n",
    "# extract\n",
    "data = masker.fit_transform(func_img, confounds=confounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the time by region array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 12))\n",
    "plt.imshow(data, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, each row is a timepoint (or volume), and each column is a region. You can see fluctuations of activity at certain timepoints. Some of these fluctuations seem to be shared by only some regions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing functional connectivity\n",
    "\n",
    "### 3.1 Example correlation\n",
    "\n",
    "Functional connectivity is essentially computing correlations between each column in the matrix. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get two regions and compute correlation\n",
    "region1 = data[:, 0]\n",
    "region2 = data[:, 1]\n",
    "r, p = stats.pearsonr(region1, region2)\n",
    "\n",
    "# multipanel figure (axes is now an array)\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 3))\n",
    "# unpack axes\n",
    "ax1, ax2 = axes\n",
    "# timeseries plot\n",
    "ax1.plot(range(data.shape[0]), region1) # blue\n",
    "ax1.plot(range(data.shape[0]), region2) # orange\n",
    "ax1.set(xlabel='Volume', ylabel='Signal (z)', title='Timeseries')\n",
    "# correlation plot\n",
    "ax2.scatter(region1, region2, alpha=.5, c='C7')\n",
    "ax2.set(xlabel='Region 1 (z)', ylabel='Region 2 (z)', title=f'r = {r:.3f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left, we see the timeseries for both regions. The right shows the scatter plot of both region, where each datapoint is a volume/timepoint. \n",
    "\n",
    "### 3.2 Correlation or connectivity Matrix\n",
    "\n",
    "We can easily expand this to all possible combinations of correlations in our data by using nilearn's `ConnectivityMeasure` ([see documentation](https://nilearn.github.io/modules/generated/nilearn.connectome.ConnectivityMeasure.html#nilearn.connectome.ConnectivityMeasure)). Calling the `fit_transform` method, as we've seen before, returns a _connectivity matrix_ showing each pairwise correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "# analyze our matrix as a list because ConnectivityMeasure expects multi-subject data\n",
    "cmat = connect.fit_transform([data])\n",
    "# get the connectivity matrix out of the list\n",
    "cmat = cmat[0]\n",
    "\n",
    "# show the shape\n",
    "cmat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plotting.plot_matrix(cmat, labels=labels, figure=fig, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a 200x200 connectivity matrix, and the overwhelming majority of correlations are positive. Let's take a look at a submatrix to break this large matrix down and understand how to interpret it. We'll take the first 9 regions, which correspond to the left visual network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_vis = cmat[:9, :9]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "plotting.plot_matrix(left_vis, labels=labels[:9], figure=fig, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_img = image.math_img(\"np.where(np.isin(img, np.arange(1, 10)), img, 0)\", img=atlas_img)\n",
    "plotting.plot_roi(vis_img, vmax=10, vmin=1, cmap='tab10', colorbar=True,\n",
    "                  cut_coords=[0, -10, -20, -30, -40, -50], display_mode='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now makes total sense as to why `17Networks_LH_Vis_1` is so different from the rest; it is really part of the parahippocampus rather than visual cortex. \n",
    "\n",
    "### 3.3 Sorting a connectivity matrix\n",
    "\n",
    "Returning to the full connectivity matrix, we can also see some networks emerge, as well as strong interhemispheric connectivity (shown by large 'squares' in the off-diagonal). Like we did with RSA representational dissimilarity matrices, we can also show a connectivity map that has been clustered based on the correlations. This essentially sorts our connectivity matrix into networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plotting.plot_matrix(cmat, labels=labels, figure=fig, vmin=-1, vmax=1, reorder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thresholding\n",
    "\n",
    "Often we want retain correlations that fall above a certain threshold. We can threshold matrices in a variety of different ways. Of course, you may choose to convert your _r_ values to _p_ values, and then apply statistical thresholding (e.g., _p_ < .05 or _p_ < .01) and then do some sort of multiple comparisons correction (e.g., false-discovery rate correction).\n",
    "\n",
    "### 4.1 Absolute thresholding\n",
    "\n",
    "A more conventional and simpler approach is to threshold at _r_ = .3. I'm not totally sure _why_ this is the cutoff, but it is sensible because _r_ values above .3 are generally considered to be moderate effect size. In our case here, _r_ > .3 corresponds to $p < 10^{-14}$ (where _n_ = 611, the number of volumes/timepoints). To do this, we can use `threshold_absolute` from `bctpy`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .3\n",
    "thresh_mat = bct.threshold_absolute(cmat, threshold)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plotting.plot_matrix(thresh_mat, labels=atlas['labels'], figure=fig, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Proportional thresholding\n",
    "\n",
    "We can also threshold based on the top proportion of correlations. For instance, we can select the correlations that fall within the 80th percentile (i.e. top 20%). We can use the `threshold_proportional` function from `bctpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .2\n",
    "thresh_mat = bct.threshold_proportional(cmat, threshold)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plotting.plot_matrix(thresh_mat, labels=atlas['labels'], figure=fig, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much sparser, which may be useful for certain applications.\n",
    "\n",
    "### 4.3 Binarizing a thresholded matrix\n",
    "\n",
    "We can also binarize our connectivity matrix (convert every nonzero value to 1). Binarization is necessary for some graph theoretical methods that we'll explore in the next tutorial. We can call the `binarize` function on one of our thresholded matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mat = bct.binarize(thresh_mat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plotting.plot_matrix(binary_mat, labels=atlas['labels'], figure=fig, cmap='binary', colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
