{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVPA 1: Pattern Classification\n",
    "\n",
    "This tutorial runs a simple pattern classification (decoding) analysis on the localizer data. We will be using many of the tools we've used so far, and also [scikit-learn](https://scikit-learn.org/stable/index.html) for pattern classification. Scikit-learn is an amazing machine learning package that is widely used. We will import some classifiers as well as ways to train these algorithms on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn.maskers import MultiNiftiMasker, NiftiMasker\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_roi\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our ROI mask\n",
    "\n",
    "Like in many decoding analyses, we'll look at patterns of voxels from a predetermined ROI (there are exceptions to this, see Searchlight). \n",
    "\n",
    "For examples sake, we'll use somatosensory cortex, SSc. Remember last time we showed that SSc had similar mean activity for Arm, Grasp, and Touch. To show this, we tossed away a lot of information by averaging across voxels. Pattern classification let's us ask a more refined question: are the _patterns_ of voxel activity in SSc different between these conditions, despite their similarities in mean? Here, we are making use of all activity in the ROI. \n",
    "\n",
    "First, let's load in our mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = nib.load('L_SSc.nii.gz')\n",
    "roi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mask is in MNI space (as is our data), but is in 2mm isovoxel. Our data, meanwhile, is in 3x3x4 MNI space. Let's load in our functional images and check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all motor task runs of one subject\n",
    "glob_template = ('data/derivatives/fmriprep/sub-03/func/'\n",
    "                '*task-motor*MNI152NLin2009cAsym*preproc_bold.nii.gz')\n",
    "func_data = glob.glob(glob_template)\n",
    "\n",
    "# reorder these based on run number \n",
    "func_imgs = sorted(list(func_data))\n",
    "func_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.load(func_imgs[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resample our mask so that it has the same number of voxels as our functional data. We are doing this so that we analyze data in according to the functional resolution rather than the mask resolution. This means fewer, and less redundant, voxels (no upsampling to mask resolution) for our pattern classifier to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `nearest` ensures that voxels are either 0 or 1 after interpolation\n",
    "roi = image.resample_to_img('L_SSc.nii.gz', func_imgs[0], interpolation='nearest')\n",
    "plot_roi(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting voxel data from our ROI\n",
    "\n",
    "Now we need to extract voxel data from our ROI. We can use `MultiNiftiMasker` to do this, which lets us pass in a list of functional (unlike `NiftiMasker`). \n",
    "\n",
    "Note here that we'll apply some temporal filtering on the data, and regress out residual head motion effects in the data. This is unlike with our first level analysis, where we applied this step as regressors in our model. We'll also standardize the voxels.\n",
    "\n",
    "Let's get those head motion regressors first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_template = ('data/derivatives/fmriprep/sub-03/func/'\n",
    "                '*task-motor*confounds_regressors.tsv')\n",
    "conf_files = glob.glob(glob_template)\n",
    "\n",
    "# reorder these based on run number\n",
    "conf_files = sorted(list(conf_files))\n",
    "\n",
    "# load only the 6 motion parameters into python\n",
    "motion_params = []\n",
    "for conf in conf_files:\n",
    "    conf_data = pd.read_table(conf, usecols=['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z'])\n",
    "    motion_params.append(conf_data.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = MultiNiftiMasker(\n",
    "    mask_img=roi, high_pass=.01, detrend=True, t_r=2, standardize=True\n",
    ")\n",
    "\n",
    "func_data = masker.fit_transform(func_imgs, confounds=motion_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what we have using matplotlib's `imshow`. This will let us plot the ROI's voxels during one functional run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 12))\n",
    "ax.imshow(func_data[0])\n",
    "ax.set(xlabel='Voxels', ylabel='Trials');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting trial patterns \n",
    "\n",
    "Now we need to only **one pattern of voxels per trial**. There are lots of ways of doing this, and several papers that explore how to apply GLM's in order to estimate single trial patterns (e.g., see the excellent work by Mumford et al: [1](https://www.sciencedirect.com/science/article/abs/pii/S1053811911010081), [2](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126255)). \n",
    "\n",
    "For the sake of an excercise/tutorial, we will instead take a pattern roughly corresponding to the peak BOLD activity in a 20s block. This is not an _ideal_ way because we're selecting one timepoint of voxels (so there will be lots of noise), but nevertheless a simple way. \n",
    "\n",
    "So, we have to a) read in the event files, b) select a volume after block/trial onset, and c) use that volume number as a row index for the voxel array above. \n",
    "\n",
    "Let's get our event files first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = glob.glob('data/task-motor*events.tsv')\n",
    "event_files = sorted(list(event_files))\n",
    "\n",
    "events = []\n",
    "for i, ev in enumerate(event_files):\n",
    "    events.append(pd.read_table(ev))\n",
    "\n",
    "# show the first 10 rows\n",
    "events[0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that gives us the volume (row) we wish to use for pattern classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_volumes(df, shift=5):\n",
    "    \n",
    "    df = df.copy()\n",
    "    # convert seconds to volume\n",
    "    df['onset_vols'] = df['onset'] / 2\n",
    "    \n",
    "    # get volume corresponding to onset + shift. Default is\n",
    "    # 5 to approximate peak reponse for a 20s block. We essentially\n",
    "    # just want something representative of the entire block\n",
    "    df['trial_vol'] = df['onset_vols'] + shift\n",
    "    \n",
    "    # convert to integers rather than floats\n",
    "    df['trial_vol'] = df['trial_vol'].astype(int)\n",
    "    \n",
    "    # returns a dataframe with 3 columns\n",
    "    return df[['trial', 'trial_type', 'trial_vol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use one event file to see what our function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run_trials = get_trial_volumes(events[0])\n",
    "first_run_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But typically, we only want two conditions of interests for our classifier in MVPA (i.e. binary classification). So we actually need to pull out just the patterns belonging to these conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_vs_touch = first_run_trials.query(\"trial_type == 'Arm' or trial_type == 'Touch'\")\n",
    "arm_vs_touch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting those volumes and plotting the voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = func_data[0]\n",
    "trial_voxels = first_run[arm_vs_touch['trial_vol'], :]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(trial_voxels)\n",
    "ax.set(xlabel='Voxels', ylabel='Trials');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I've created a handy function that does this across runs. It iterates through each run, and gives you a final voxel array and row labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_voxels(voxels, task_events, conditions):\n",
    "    # iterate through data and event_files in tandem\n",
    "    \n",
    "    cond1, cond2 = conditions\n",
    "    \n",
    "    trials_list = []\n",
    "    data_list = []\n",
    "    for i, (vox, ev) in enumerate(zip(voxels, task_events)):\n",
    "        \n",
    "        # get the trial patterns for specific conditions we want to use\n",
    "        trials = get_trial_volumes(ev)\n",
    "        trials = trials.query(\"trial_type == @cond1 or trial_type == @cond2\")\n",
    "        data = vox[trials.loc[:, 'trial_vol'], :]\n",
    "        \n",
    "        # get condition labels from trial type column\n",
    "        trials.loc[:, 'run'] = i\n",
    "        \n",
    "        trials_list.append(trials)\n",
    "        data_list.append(data)\n",
    "    \n",
    "    # concatenate lists to create single dataframe and voxel array\n",
    "    trials = pd.concat(trials_list)\n",
    "    data = np.vstack(data_list)\n",
    "        \n",
    "    return trials, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run it, and it returns information on the trial (trial type, number, etc) and the voxels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, voxels = get_trial_voxels(func_data, events, ['Arm', 'Touch'])\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(voxels)\n",
    "ax.set(xlabel='Voxels', ylabel='Trials');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running a classifier\n",
    "\n",
    "Now we can use [scikit learn](https://scikit-learn.org/stable/index.html) to train and evaluate our model. First, we need to define our classifier. At the top, we imported `SVC`, which is support vector machine (SVM) classifier, and `LogisticRegression`, which is a regularized logistic regression classifier. These are probably the two most popular algorithms in decoding studies as the tend to perform well (SVM generally moreso than logistic regression). \n",
    "\n",
    "\n",
    "**SVMs:**  \n",
    "A SVM essentially tries to find a hyperplane between classes with maximal margins. The margins are defined by the closest points to the hyperplane. This is easy to visualize in with two features (e.g., two voxels):\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_0011.png)\n",
    "\n",
    "The figure above is taken from the excellent scikit-learn documentation: https://scikit-learn.org/stable/modules/svm.html#svm-classification.\n",
    "\n",
    "In our case, were trying to find a hyperplane in _N_-dimensional space, where _N_ = number of voxels. SVMs perform very well on the data, as they're fairly robust to noise and redundancy.\n",
    "\n",
    "**Logistic Regression:**  \n",
    "Scikit learn implements a regularized ($\\ell_2$-norm) logistic regression by default (which differs from your standard logistic regression model that has no regularization). Logistic regression essentially transforms the output of a linear regression using a sigmoid function, which produces probabilities between 0 and 1. Binary classification is just done by taking probability values below or above 50% and assigning them as 0 or 1, respectively. \n",
    "\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_logistic_001.png)\n",
    "\n",
    "The graph above is created by this example: https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py\n",
    "\n",
    "See also: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "\n",
    "Next, we need to set up a way to train our model and then evaluate it. In supervised machine learning, models are first fit on training data. This process essentially lets the model map input data to labels/outcomes. Then the trained models are tested on unseen data to try and predict their labels/outcomes. In our case, we will train our model on voxel data from two conditions, and then use a held-out, unseen test set to try and predict the condition of those data based on their voxels.  \n",
    "\n",
    "We can do this using [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). Cross validation is an iterative process in which we split our data into train and test folds. Each iteration involves a unique split, thus letting us train and test across all of our data. For each iteration, we obtain a classification accuracy, and we can compute an average accuracy across iterations. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1280px-K-fold_cross_validation_EN.svg.png)\n",
    "\n",
    "Scikit-learn makes this process increadibly simple using `cross_val_score`. This does everything for us. All we need to do is give it our data, our model, and how we want to cross validate and score the procedure (e.g., classification accuracy).\n",
    "\n",
    "So, let's make sure we're clear on our data. Lets set up `X` and `y`, which is typical notation you'll see in scikit-learn docs and elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "info, X = get_trial_voxels(func_data, events, ['Arm', 'Touch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = info['trial_type'].tolist()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up our cross validator. How do we determine our splits? People typically use _K_-fold cross validation shown above, but in neuroimaging, we often use _leave-one-run-out_ cross validation. This essentially lets our functional runs determine our folds, and ensures that our train/test folds are completely independent from one another (which reduces overfitting by reducing shared variance between train and test sets). \n",
    "\n",
    "Let's run `cross_val_score` and give it our model and the data. Note that `groups` is used to define our functional runs (i.e. folds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(svm, X, y, groups=info['run'], cv=LeaveOneGroupOut())\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Group Analysis\n",
    "\n",
    "We can put everything we've done into one big for-loop that iterates across subjects. For each subject, we'll run the pattern classification pipeline above and take their mean accuracy. Then we can show these accuracies at the group level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "subject_numbers = ['01', '02', '03', '04', '05', '06', '07', '08']\n",
    "columns = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "roi = 'L_SSc.nii.gz'\n",
    "conditions = ['Arm', 'Touch']\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# resample our mask\n",
    "roi = image.resample_to_img(roi, func_imgs[0], interpolation='nearest')\n",
    "\n",
    "results = []\n",
    "for subject in subject_numbers:\n",
    "    \n",
    "    ## 1. Get the functional images in order\n",
    "    glob_template = (f'data/derivatives/fmriprep/sub-{subject}/func/'\n",
    "                     '*task-motor*MNI152NLin2009cAsym*preproc_bold.nii.gz')\n",
    "    func_imgs = glob.glob(glob_template)\n",
    "    func_imgs = sorted(list(func_imgs))\n",
    "    \n",
    "    # 2. Get the regressors\n",
    "    glob_template = (f'data/derivatives/fmriprep/sub-{subject}/func/'\n",
    "                      '*task-motor*confounds_regressors.tsv')\n",
    "    conf_files = glob.glob(glob_template)\n",
    "    # put files in order\n",
    "    conf_files = sorted(list(conf_files))\n",
    "\n",
    "    # load only the 6 motion parameters\n",
    "    motion_params = []\n",
    "    for conf in conf_files:\n",
    "        conf_data = pd.read_table(conf, usecols=columns)\n",
    "        motion_params.append(conf_data.values)\n",
    "        \n",
    "    # extract data\n",
    "    masker = MultiNiftiMasker(mask_img=roi, high_pass=.01, detrend=True, t_r=2, \n",
    "                              standardize=True)\n",
    "    func_data = masker.fit_transform(func_imgs, confounds=motion_params)\n",
    "    \n",
    "    # get trial patterns with trial information\n",
    "    info, voxels = get_trial_voxels(func_data, events, conditions)\n",
    "    \n",
    "    # run cross validation\n",
    "    X = voxels\n",
    "    y = info['trial_type'].tolist()\n",
    "    accuracies = cross_val_score(clf, X, y, groups=info['run'], cv=LeaveOneGroupOut())\n",
    "    \n",
    "    # save subject mean accuracy\n",
    "    results.append(np.mean(accuracies))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into dataframe that's convenient for seaborn\n",
    "df = pd.DataFrame({'x': '', 'accuracy': results})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 6))\n",
    "sns.barplot(x='x', y='accuracy', data=df, ci=68, ax=ax, color='C7')\n",
    "sns.stripplot(x='x', y='accuracy', data=df, ax=ax, color='k',\n",
    "              edgecolor='k', marker='o', size=4)\n",
    "ax.set(ylim=(.4, 1), xlabel='')\n",
    "ax.axhline(.5, c='k', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classification accuracy is above chance value of 50%! It's not amazing, but this would pass in a publication. Remember, we're taking _very_ noisy and crude signals of neural activity and asking if we can see any meaningful patterns in them. The fact that we can in the first place is impressive. \n",
    "\n",
    "If this lecture was a lot, that's okay. This is a heavy-duty topic that is difficult to grasp in a quick tutorial. The big take away is to know that this approach exists, and to get a quick glimpse of what goes into it when you read papers or decide on analyses in your own projects.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psyc917-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
